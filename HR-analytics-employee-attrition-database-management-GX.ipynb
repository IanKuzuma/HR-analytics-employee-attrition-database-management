{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a04fec7",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda5d4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Name**                : Ladityarsa Ilyankusuma\n",
    "\n",
    "**E-Mail**               : ladityarsa.ian@gmail.com\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca66b9",
   "metadata": {},
   "source": [
    "### A. Datasource Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ee4b0",
   "metadata": {},
   "source": [
    "##### a.1. Source Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f156952",
   "metadata": {},
   "source": [
    "Dataset Kaggle Source: [IBM HR Analytics Employee Attrition & Performance](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce6b53",
   "metadata": {},
   "source": [
    "##### a.2. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b977cc2",
   "metadata": {},
   "source": [
    "This datasource provides a detailed snapshot of employees within a corporate environment, covering a wide range of personal, professional, and performance-related attributes. It includes demographic details such as age, gender, and education level, as well as job-specific data like department, role, salary, and work experience.\n",
    "\n",
    "A key focus of this dataset is employee attrition or whether someone has left the company or not, making it especially useful for analyzing turnover trends. It also contains indicators of workload and satisfaction, such as overtime status, job involvement, and work-life balance ratings. These features together allow for rich exploration into what factors might influence employee retention, engagement, and productivity.\n",
    "\n",
    "The dataset is well-suited for HR analytics to explore important questions such as \"show me a breakdown of distance from home by job role and attrition\" or \"compare average monthly income by education and attrition\", and decision-making support in areas like workforce planning, employee well-being initiatives, and performance management strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef2649",
   "metadata": {},
   "source": [
    "##### a.3. General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa5d27",
   "metadata": {},
   "source": [
    "- Rows: 1,470\n",
    "- Columns: 35\n",
    "- Missing Values: 0\n",
    "- Duplicate Rows: 0\n",
    "\n",
    "This indicates the dataset is in a good state before validation, all column names are normalized and no nulls nor duplicates were found after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23bf1c",
   "metadata": {},
   "source": [
    "**Column Types Breakdown**\n",
    "\n",
    "| Data Type            | Count |\n",
    "| -------------------- | ----- |\n",
    "| Integer (numerical)  | 19    |\n",
    "| Object (categorical) | 16    |\n",
    "\n",
    "Most of the features are numeric and suitable for statistical validations, while categorical fields are candidates for set and type checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d371c0",
   "metadata": {},
   "source": [
    "**Example of Numerical Columns**\n",
    "\n",
    "- `age`\n",
    "- `daily_rate`\n",
    "- `distance_from_home`\n",
    "- `monthly_income`\n",
    "- `num_companies_worked`\n",
    "- `percent_salary_hike`\n",
    "- `total_working_years`\n",
    "- `years_at_company`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77284650",
   "metadata": {},
   "source": [
    "**Example of Categorical Columns**\n",
    "\n",
    "- `attrition`\n",
    "- `business_travel`\n",
    "- `department`\n",
    "- `education`\n",
    "- `gender`\n",
    "- `job_role`\n",
    "- `marital_status`\n",
    "- `over_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477924e6",
   "metadata": {},
   "source": [
    "### B. Objective Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce3c2a",
   "metadata": {},
   "source": [
    "This notebook is part of a data quality assurance workflow using **Great Expectations (GX)** to validate a structured HR dataset before further analysis. The dataset contains detailed employee records, including job roles, compensation, work-life balance, and performance metrics, all of which are essential for exploring trends in attrition and workforce behavior.\n",
    "\n",
    "##### b.1. Why Data Validation?\n",
    "\n",
    "Before generating insights or building machine learning models, it is critical to **ensure that the underlying data is trustworthy, clean, and consistent**. This notebook uses Great Expectations to:\n",
    "\n",
    "* Check the integrity of columns (such as uniqueness, value ranges, allowed types)\n",
    "* Catch subtle data issues early (like invalid categories, duplicated rows, or out-of-range values)\n",
    "* Provide a documented, automated validation layer that can be rerun as data pipelines evolve\n",
    "\n",
    "This validation is especially important in the context of HR analytics, where **small data quality issues can mislead strategic decisions** about hiring, retention, or compensation.\n",
    "\n",
    "##### b.2. What This Notebook Covers\n",
    "\n",
    "Using Great Expectations, we perform the following tasks:\n",
    "\n",
    "1. **Connect the HR dataset** (stored as a CSV) to a Pandas-based GX Datasource\n",
    "2. **Create or reuse a reusable expectation suite** tailored to this dataset\n",
    "3. **Apply 7 carefully selected expectations**, including:\n",
    "\n",
    "   * Uniqueness checks for employee identifiers\n",
    "   * Valid value ranges for numerical columns like `age` or `monthly_income`\n",
    "   * Membership constraints for categorical features like `education`\n",
    "   * Correct data form of integer or float for numerical columns like `daily_rate`\n",
    "   * Valid values of two intertwined columns like `monthly_rate` and `daily_rate`\n",
    "   * Make sure there are no missing chunks of data or data duplications in the pipeline\n",
    "4. **Run a validation checkpoint** to evaluate the dataset against the expectation suite\n",
    "5. **Generate and view Data Docs**, a visual report that summarizes which expectations passed or failed\n",
    "\n",
    "These steps help ensure that any downstream analytics, whether in dashboards, KPIs, or predictive models, are built on reliable data.\n",
    "\n",
    "##### b.3. Business Relevance\n",
    "\n",
    "While the notebook focuses on **technical validation**, the ultimate goal is **business reliability**. Clean HR data allows analysts and stakeholders to:\n",
    "\n",
    "* Spot early warning signs of employee disengagement\n",
    "* Understand how performance varies across roles or departments\n",
    "* Identify imbalances in workload, compensation, or satisfaction\n",
    "* Support HR policies with evidence-driven insights\n",
    "\n",
    "By embedding validation directly into the data workflow, this notebook ensures that these insights are not only useful, but also **credible**.\n",
    "\n",
    "##### b.4. Intended Users\n",
    "\n",
    "* **Data analysts and data engineers** responsible for maintaining clean HR data pipelines\n",
    "* **HR analysts** validating employee datasets before visual exploration\n",
    "* **People analytics teams** building predictive models or executive dashboards\n",
    "* **Decision-makers** who rely on accurate, validated data to drive workforce strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee95959",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ef3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "## pip install -q \"great-expectations==0.18.19\"\n",
    "from great_expectations.data_context import FileDataContext\n",
    "import great_expectations.exceptions as gx_exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea529f",
   "metadata": {},
   "source": [
    "# 3. Instantiate a Great Expectations Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38570e4",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "\n",
    "This step creates a **new Great Expectations (GX) context**, basically the core environment GX uses to store and manage all your validation assets (datasources, expectation suites, checkpoints, and data docs). We're saving this context inside an auto-generated local folder called `/gx`.\n",
    "\n",
    "**This is where GX tracks everything:**\n",
    "\n",
    "* What data sources you've connected\n",
    "* What expectations you've defined\n",
    "* Where to log validation results\n",
    "* How to build visual reports (Data Docs)\n",
    "\n",
    "**Having a clearly defined data context allows us to:**\n",
    "\n",
    "* Run repeatable validation checks every time new data comes in\n",
    "* Easily organize and persist your expectations and reports across notebook restarts\n",
    "* Avoid relying on memory or temp configs, this project becomes modular and portable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d16c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new GX context in the current folder\n",
    "context = FileDataContext.create(project_root_dir=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60243171",
   "metadata": {},
   "source": [
    "# 4. Connect to a Datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53992e79",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "\n",
    "This step is about telling Great Expectations **where our data lives** and how to read it.\n",
    "\n",
    "1. `add_pandas(...)` registers a datasource using Pandas, ideal for working with local `.csv` files in-memory.\n",
    "2. If the datasource or asset already exists (like after a kernel restart), we catch that gracefully and re-load it from the context using a `try/except` block.\n",
    "3. `add_csv_asset(...)` connects a specific CSV file as a data asset under the datasource.\n",
    "4. Finally, we generate a `batch_request`, which is GX's internal format for pulling the actual data when running validations.\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "* Datasources are GX’s entry point into **any data system**: files, databases, cloud storage, etc.\n",
    "* Assets represent **specific datasets** within a datasource.\n",
    "* Batch requests are the link between your raw data and your expectations, they’re used to validate and explore real data batches.\n",
    "\n",
    "The use of `try/except` makes this notebook **resilient to kernel restarts** or reruns, avoiding “already exists” errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the datasource. This name must be unique between Datasources.\n",
    "datasource_name = \"csv_datasource_m3\"\n",
    "\n",
    "# Add a Pandas-based datasource, or load it if it's already exist\n",
    "try:\n",
    "    datasource = context.sources.add_pandas(datasource_name)\n",
    "except gx_exceptions.DatasourceError:\n",
    "    datasource = context.sources[datasource_name]\n",
    "\n",
    "# Define the name of the asset and its path\n",
    "asset_name = \"m3_data_clean\"\n",
    "path_to_data = \"./data/HR_employee_attrition_dataset_clean.csv\"\n",
    "\n",
    "# Add the CSV file as a data asset, or load it if it's already exist\n",
    "try:\n",
    "    asset = datasource.add_csv_asset(asset_name, filepath_or_buffer=path_to_data)\n",
    "except gx_exceptions.DataAssetError:\n",
    "    asset = datasource.assets[asset_name]\n",
    "\n",
    "# Build the batch request used to fetch the data for validation\n",
    "batch_request = asset.build_batch_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035da1c2",
   "metadata": {},
   "source": [
    "# 5. Create an Expectation Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a2cfa",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "\n",
    "This step sets up the **expectation suite**, which is where our data rules (validations) will be saved.\n",
    "\n",
    "1. `add_expectation_suite()` creates a new empty suite named `\"m3_suite\"`.\n",
    "2. If that suite already exists (like after re-running the notebook), we load it instead using `get_expectation_suite()`.\n",
    "3. Then, we create a `validator`, a central GX object that links:\n",
    "\n",
    "   * The actual data (via `batch_request`)\n",
    "   * The validation logic (via the suite)\n",
    "\n",
    "Finally, `validator.head()` gives a preview of the data you're about to validate, which helps ensure the connection works and the data looks right.\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "* An **expectation suite** is like a checklist for what “good” data should look like: column types, value ranges, uniqueness, etc.\n",
    "* The **validator** is where you define and apply those rules interactively in your notebook.\n",
    "* All expectations you define later will be **added to this suite**.\n",
    "\n",
    "The `try/except` again ensures the suite can be reused across multiple notebook runs without throwing duplicate errors, keeping the workflow smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade4ad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77aa2fddc5444587a74bb522ca879e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>attrition</th>\n",
       "      <th>business_travel</th>\n",
       "      <th>daily_rate</th>\n",
       "      <th>department</th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>education</th>\n",
       "      <th>education_field</th>\n",
       "      <th>employee_count</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_satisfaction</th>\n",
       "      <th>standard_hours</th>\n",
       "      <th>stock_option_level</th>\n",
       "      <th>total_working_years</th>\n",
       "      <th>training_times_last_year</th>\n",
       "      <th>work_life_balance</th>\n",
       "      <th>years_at_company</th>\n",
       "      <th>years_in_current_role</th>\n",
       "      <th>years_since_last_promotion</th>\n",
       "      <th>years_with_curr_manager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>College</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Low</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Bad</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>Below College</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Very High</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Better</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>College</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>Better</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>Master</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>High</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>Better</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>Below College</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Very High</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Better</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age attrition    business_travel  daily_rate              department  \\\n",
       "0   1   41       Yes      Travel_Rarely        1102                   Sales   \n",
       "1   2   49        No  Travel_Frequently         279  Research & Development   \n",
       "2   4   37       Yes      Travel_Rarely        1373  Research & Development   \n",
       "3   5   33        No  Travel_Frequently        1392  Research & Development   \n",
       "4   7   27        No      Travel_Rarely         591  Research & Development   \n",
       "\n",
       "   distance_from_home      education education_field  employee_count  ...  \\\n",
       "0                   1        College   Life Sciences               1  ...   \n",
       "1                   8  Below College   Life Sciences               1  ...   \n",
       "2                   2        College           Other               1  ...   \n",
       "3                   3         Master   Life Sciences               1  ...   \n",
       "4                   2  Below College         Medical               1  ...   \n",
       "\n",
       "  relationship_satisfaction standard_hours  stock_option_level  \\\n",
       "0                       Low             80                   0   \n",
       "1                 Very High             80                   1   \n",
       "2                    Medium             80                   0   \n",
       "3                      High             80                   0   \n",
       "4                 Very High             80                   1   \n",
       "\n",
       "  total_working_years  training_times_last_year work_life_balance  \\\n",
       "0                   8                         0               Bad   \n",
       "1                  10                         3            Better   \n",
       "2                   7                         3            Better   \n",
       "3                   8                         3            Better   \n",
       "4                   6                         3            Better   \n",
       "\n",
       "  years_at_company years_in_current_role  years_since_last_promotion  \\\n",
       "0                6                     4                           0   \n",
       "1               10                     7                           1   \n",
       "2                0                     0                           0   \n",
       "3                8                     7                           3   \n",
       "4                2                     2                           2   \n",
       "\n",
       "   years_with_curr_manager  \n",
       "0                        5  \n",
       "1                        7  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the name of the suite\n",
    "suite_name = \"m3_suite\"\n",
    "\n",
    "# Create the expectation suite, or get it if it's already exist\n",
    "try:\n",
    "    suite = context.add_expectation_suite(expectation_suite_name=suite_name)\n",
    "except gx_exceptions.DataContextError:\n",
    "    suite = context.get_expectation_suite(expectation_suite_name=suite_name)\n",
    "\n",
    "# Create the validator using above expectation suite\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request, \n",
    "    expectation_suite=suite)\n",
    "\n",
    "# Check the validator\n",
    "validator.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06c957",
   "metadata": {},
   "source": [
    "### A. Define Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1526c8",
   "metadata": {},
   "source": [
    "Once the expectation suite is initialized and the validator is ready, this is where we **define the specific rules** that our dataset must follow, or more famously known as **expectations** in Great Expectations.\n",
    "\n",
    "Expectations are the heart of data validation. **They allow us to:**\n",
    "\n",
    "* Ensure data consistency and quality across pipelines\n",
    "* Catch schema drifts or anomalies early\n",
    "* Document data contracts in a human-readable way\n",
    "\n",
    "Each expectation reflects a business or technical assumption we want to enforce. In this project, we've designed **7 key expectations** based on column-level semantics and domain understanding. Each expectation is added interactively through the `validator` and automatically saved into the suite (`m3_suite`) for future validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2113cd",
   "metadata": {},
   "source": [
    "##### a.1. Expectation 1 : `to be unique`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22415e",
   "metadata": {},
   "source": [
    "This expectation verifies that the `id` column contains **unique values**, ensuring that each row in the dataset represents a distinct employee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9d88fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b3f3194d8f465d9bff467e495f6438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {\n",
       "    \"element_count\": 1470,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"partial_unexpected_list\": [],\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"unexpected_percent_nonmissing\": 0.0\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation 1 : Column `id` must be unique\n",
    "validator.expect_column_values_to_be_unique(column='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3ebbe",
   "metadata": {},
   "source": [
    "**Purpose of the Expectation:**\n",
    "\n",
    "In any employee-related dataset, especially those used for HR analytics or people insights, the `id` field typically acts as a **primary key**. Enforcing uniqueness here is essential for several reasons:\n",
    "\n",
    "* It prevents data duplication, which could skew metrics such as attrition rates or average tenure.\n",
    "* It ensures consistency when joining with other datasets, like performance reviews or payroll.\n",
    "* It enables accurate row-level analysis, such as tracking an individual’s progression or behavior over time.\n",
    "\n",
    "**Result Summary:**\n",
    "\n",
    "* `element_count: 1470` — All 1,470 rows were checked.\n",
    "* `unexpected_count: 0` — No duplicate `id` values were found.\n",
    "* `unexpected_percent: 0.0` — 0% of the rows failed this expectation.\n",
    "* `missing_count: 0` — There were also no null or missing values in the `id` column.\n",
    "\n",
    "These results indicate that the dataset successfully meets the uniqueness requirement. Every employee in the dataset is uniquely identified, which gives us a solid foundation for all downstream analysis and ensures trust in any insights derived from this data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee838ef5",
   "metadata": {},
   "source": [
    "##### a.2. Expectation 2 : `to be between min_value and max_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b16b78",
   "metadata": {},
   "source": [
    "This expectation ensures that all values in the `age` column fall within a **realistic and valid working age range** of 18 to 60 years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199759a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed63b39806847e587ef345c1dbd3903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {\n",
       "    \"element_count\": 1470,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"partial_unexpected_list\": [],\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"unexpected_percent_nonmissing\": 0.0\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation 2 : Column `age` must be between 18 and 60\n",
    "validator.expect_column_values_to_be_between(\n",
    "    column='age', min_value=18, max_value=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f1cc7",
   "metadata": {},
   "source": [
    "**Purpose of the Expectation:**\n",
    "\n",
    "Validating age boundaries is crucial in workforce datasets to:\n",
    "\n",
    "* **Catch outliers** or data entry errors (ages below 18 or unrealistically high like 95).\n",
    "* **Align the dataset with employment regulations**, which often stipulate minimum and maximum legal working ages.\n",
    "* **Ensure accuracy** in segmentation and modeling, such as when age groups are used to study engagement, compensation trends, or attrition patterns.\n",
    "\n",
    "By enforcing this range, we help ensure the demographic data is both clean and compliant.\n",
    "\n",
    "**Result Summary:**\n",
    "\n",
    "* `element_count: 1470` — All rows in the dataset were checked.\n",
    "* `unexpected_count: 0` — Every `age` value was within the valid range (18–60).\n",
    "* `unexpected_percent: 0.0` — No violations were found.\n",
    "* `missing_count: 0` — The column contains no nulls or blanks.\n",
    "\n",
    "This successful result confirms that the `age` column is both complete and logically valid. This allows us to confidently use this feature in further analysis tasks without needing additional filtering or imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa427d",
   "metadata": {},
   "source": [
    "##### a.3. Expectation 3 : `to be in set`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55513d80",
   "metadata": {},
   "source": [
    "This expectation checks that the `education` column **only contains valid categorical labels** from a predefined set of accepted educational levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ade49af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4e17c28a0f4d3f8cda22c55bcad7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {\n",
       "    \"element_count\": 1470,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"partial_unexpected_list\": [],\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"unexpected_percent_nonmissing\": 0.0\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation 3 : Column `education` must contain one of the following 5 things:\n",
    "## - 'Below College'\n",
    "## - 'College'\n",
    "## - 'Bachelor'\n",
    "## - 'Master'\n",
    "## - 'Doctor'\n",
    "validator.expect_column_values_to_be_in_set(\n",
    "    'education', \n",
    "    ['Below College', 'College', 'Bachelor', 'Master', 'Doctor']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f40d3",
   "metadata": {},
   "source": [
    "**Purpose of the Expectation:**\n",
    "\n",
    "Ensuring categorical consistency is essential in HR analytics. By restricting values in the `education` column to a controlled set of labels, we:\n",
    "\n",
    "* **Prevent typos or inconsistent entries** (like \"bachelor\" vs \"Bachelor\" or \"Masters\").\n",
    "* **Ensure correct groupings** for statistical comparisons, especially when education level is used to study performance, compensation, or promotion patterns.\n",
    "* Maintain **data integrity** for downstream dashboards and ML models that treat these categories as features.\n",
    "\n",
    "**Result Summary:**\n",
    "\n",
    "* `element_count: 1470` — All rows were evaluated.\n",
    "* `unexpected_count: 0` — Every value in `education` matched one of the expected labels.\n",
    "* `missing_count: 0` — There are no nulls or blanks in this column.\n",
    "* `unexpected_percent: 0.0` — The entire column adheres strictly to the defined set.\n",
    "\n",
    "This confirms that the `education` field is not only complete but also well-standardized, making it safe for use in grouping, aggregation, or encoding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6148e",
   "metadata": {},
   "source": [
    "##### a.4. Expectation 4 : `to be in type list`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d073d9",
   "metadata": {},
   "source": [
    "This expectation ensures that the values in the `daily_rate` column are of a **valid numerical data type**, specifically either `int64` or `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f69f06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbd8e938da544be9033897662ce7ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {\n",
       "    \"observed_value\": \"int64\"\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation 4 : Column `daily_rate` must in form of integer or float\n",
    "validator.expect_column_values_to_be_in_type_list(\n",
    "    'daily_rate', ['int64', 'float64']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2655f",
   "metadata": {},
   "source": [
    "**Purpose of the Expectation:**\n",
    "\n",
    "Data type validation is critical when preparing numerical columns for:\n",
    "\n",
    "* **Mathematical operations** (sums, averages, comparisons),\n",
    "* **Modeling** (numeric encoding),\n",
    "* And **visualization** (like histograms or scatter plots).\n",
    "\n",
    "By ensuring that `daily_rate` contains only integers or floats, we avoid issues like:\n",
    "\n",
    "* Parsing errors from string-based numbers (`'800'`, a number as a string),\n",
    "* Unexpected objects (`None`, `N/A`, or mixed types),\n",
    "* Downstream calculation bugs in aggregations or feature engineering.\n",
    "\n",
    "**Result Summary:**\n",
    "\n",
    "* `success: true` — All values in the column conform to an accepted type.\n",
    "* `observed_value: \"int64\"` — The entire column consists of 64-bit integers.\n",
    "* The fact that no mixed types were detected further confirms data consistency.\n",
    "\n",
    "This makes the `daily_rate` column safe for any form of numerical analysis requiring numeric dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f9dc5",
   "metadata": {},
   "source": [
    "##### a.5. Expectation 5 : `A to be greater than B`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b56a6a",
   "metadata": {},
   "source": [
    "This expectation enforces a **logical relationship** between two numerical columns:\n",
    "It ensures that for every row, the value in `monthly_rate` is **greater than** the value in `daily_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc72f125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f8e5d014b0431fbffb7ea8037492d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {\n",
       "    \"element_count\": 1470,\n",
       "    \"unexpected_count\": 0,\n",
       "    \"unexpected_percent\": 0.0,\n",
       "    \"partial_unexpected_list\": [],\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_percent_total\": 0.0,\n",
       "    \"unexpected_percent_nonmissing\": 0.0\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation 5 : Column `monthly_rate` must always be greater than column `daily_rate`\n",
    "validator.expect_column_pair_values_A_to_be_greater_than_B(\n",
    "    column_A='monthly_rate',\n",
    "    column_B='daily_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd8521",
   "metadata": {},
   "source": [
    "**Purpose of the Expectation:**\n",
    "\n",
    "This type of cross-column validation is useful when:\n",
    "\n",
    "* The columns represent **hierarchically related quantities**, such as daily vs. monthly compensation,\n",
    "* There’s a **business logic** that must always hold true (monthly pay > daily pay),\n",
    "* You're checking for **data entry errors** (accidentally swapped or inconsistent values).\n",
    "\n",
    "By applying this expectation, we confirm that the dataset adheres to real-world constraints, improving reliability and trust in downstream analysis.\n",
    "\n",
    "**Result Summary:**\n",
    "\n",
    "* `success: true` — Every single row satisfies the rule.\n",
    "* `unexpected_count: 0` — No violations found where daily rate exceeds or equals monthly rate.\n",
    "* This confirms the **integrity of pay structure** in the dataset and rules out any flipped or inconsistent entries in these two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc20d77",
   "metadata": {},
   "source": [
    "##### a.6. Expectation 6 : `median to be between min_value and max_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f90a60",
   "metadata": {},
   "source": [
    "This expectation validates that the **median value** of the `monthly_income` column falls within a defined, reasonable range of \\$4,000 to \\$6,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f2ca56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fd5adf9f064694952a0a875390684f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {\n",
       "    \"observed_value\": 4919.0\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation 6 : The median of column `monthly_income` must be in range of 4000 - 6000 dollars\n",
    "validator.expect_column_median_to_be_between(\n",
    "    column='monthly_income', min_value=4000, max_value=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7eb75d",
   "metadata": {},
   "source": [
    "**Purpose of the Expectation:**\n",
    "\n",
    "Validating the median is especially useful when:\n",
    "\n",
    "* You want to **summarize the central tendency** of skewed numerical data (like income),\n",
    "* There’s an expected **business benchmark** range (like typical employee salary),\n",
    "* You want to catch shifts or anomalies in data distributions over time.\n",
    "\n",
    "This expectation helps ensure that compensation data remains **plausible and within expected business norms**.\n",
    "\n",
    "**Result Summary:**\n",
    "\n",
    "* `observed_value: 4919.0` — The median monthly income is well within the expected range of \\$4000–\\$6000.\n",
    "* `success: true` — Confirms that the dataset's income distribution remains centered around typical company pay ranges.\n",
    "* This builds trust in the salary data and can help validate predictive models or dashboards that use this column as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db6e0d",
   "metadata": {},
   "source": [
    "##### a.7. Expectation 7 : `row count to be between min_value and max_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84d958",
   "metadata": {},
   "source": [
    "This expectation ensures that the **number of rows in the dataset** falls within an acceptable range, in this case between **1,000 and 2,000** records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb1f6c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b5b9de15cd4b7eb7f1983d6a8a8bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {\n",
       "    \"observed_value\": 1470\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expectation 7 : Row count must be in range of 1000 - 2000 rows\n",
    "validator.expect_table_row_count_to_be_between(\n",
    "    min_value=1000, max_value=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7237f8b",
   "metadata": {},
   "source": [
    "**Purpose of the Expectation:**\n",
    "\n",
    "Validating the total row count is a basic but powerful check to:\n",
    "\n",
    "* Confirm the dataset is **complete and loaded properly**,\n",
    "* Detect partial loads, dropped records, or duplicate imports,\n",
    "* Serve as a **guardrail** against structural issues in the pipeline or manual errors during collection.\n",
    "\n",
    "This is especially useful in periodic ETL workflows where the dataset size should remain within a consistent range over time.\n",
    "\n",
    "**Result Summary:**\n",
    "\n",
    "* `observed_value: 1470` — The dataset has 1,470 rows.\n",
    "* This value is **within the acceptable range** of 1,000 to 2,000.\n",
    "* `success: true` confirms the dataset was **fully and correctly loaded** with no missing or excess rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59cffd",
   "metadata": {},
   "source": [
    "### B. Save the Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94159ed0",
   "metadata": {},
   "source": [
    "Once we’ve defined all our validation rules (expectations), we need to **persist them into the expectation suite** so they can be reused or executed later — such as in automated data pipelines or future validation runs.\n",
    "\n",
    "**What this does:**\n",
    "\n",
    "* Saves the current state of the expectation suite (`m3_suite`) into the GX project directory (`gx/expectations/`).\n",
    "* The `discard_failed_expectations=False` flag ensures that **even expectations that might have failed** during testing are retained in the suite (though in our case, all passed).\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "* This is a critical step in **productionizing data validation**, as it turns our validation logic from ephemeral code into **a reusable contract** between data producers and consumers.\n",
    "* The saved suite can now be linked to **Checkpoints**, **Data Docs**, or even integrated into **CI/CD pipelines** for automated checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b82ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the expectations into the expectation suite\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b054d3",
   "metadata": {},
   "source": [
    "### C. Run a Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b698c",
   "metadata": {},
   "source": [
    "A **Checkpoint** in Great Expectations is a reproducible validation routine. It bundles together:\n",
    "\n",
    "* The datasource (where the data lives),\n",
    "* The batch request (what slice of data to validate),\n",
    "* The expectation suite (the rules to validate against),\n",
    "\n",
    "into a **single, versionable object** that can be executed over and over again, manually or automatically.\n",
    "\n",
    "**What this does:**\n",
    "\n",
    "* `add_or_update_checkpoint` registers a new checkpoint (or updates it if it already exists).\n",
    "* `checkpoint.run()` triggers the entire validation process defined by the expectation suite.\n",
    "* All results, including pass/fail summaries, are stored in the local GX context (`gx/uncommitted/validations/`) for tracking and reporting.\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "* This is **the moment of truth** where we see if the data meets our expectations.\n",
    "* It also **decouples validation from code**. Once defined, a checkpoint can be triggered without re-running the entire notebook logic.\n",
    "* You can later **schedule this checkpoint** in a production pipeline or in our case, Airflow DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0be6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the checkpoint\n",
    "checkpoint_name = \"checkpoint_m3\"\n",
    "\n",
    "# Create or update a persistent checkpoint using the validator we've built\n",
    "try:\n",
    "    checkpoint = context.add_or_update_checkpoint(\n",
    "        name=checkpoint_name,\n",
    "        validator=validator,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Failed to add or update checkpoint:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01f3d0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1423c46131a479d94efbd86c7ff69e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the checkpoint\n",
    "checkpoint_results = checkpoint.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd9bf5",
   "metadata": {},
   "source": [
    "### D. Build the Data Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e764b63",
   "metadata": {},
   "source": [
    "After running validations and saving expectations, the final step is to **generate a visual report** of the results. Great Expectations does this through **Data Docs**, which are static HTML pages that show which validations passed or failed.\n",
    "\n",
    "This will compile everything: datasource info, suite expectations, and validation results, into an organized visual format. We can then open it locally through our browser (if running on a local machine), or deploy it to a remote viewer for team sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95baeaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Docs have been successfully built.\n"
     ]
    }
   ],
   "source": [
    "# Build data docs (renders HTML files)\n",
    "context.build_data_docs()\n",
    "print(\"Data Docs have been successfully built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d0c9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the docs in default web browser (only works locally)\n",
    "context.open_data_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46060add",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073362d",
   "metadata": {},
   "source": [
    "We have successfully executed a complete **data validation pipeline** using **Great Expectations** on a structured HR dataset. The workflow was carefully designed to ensure that the dataset is **clean, consistent, and trustworthy** before it's used for deeper analytics or business intelligence.\n",
    "\n",
    "We started by configuring a **Great Expectations context** and connecting to a **Pandas-based datasource**, enabling direct ingestion of our cleaned CSV file. An **expectation suite** was then created and tied to a **validator**, which we used to define and apply seven targeted expectations that reflect both **data quality standards** and **business logic requirements**.\n",
    "\n",
    "### A. Summary of Validation Expectations:\n",
    "\n",
    "| # | Expectation Description       | Column(s) Involved                                               | Outcome                                          |\n",
    "| - | ----------------------------- | ---------------------------------------------------------------- | ------------------------------------------------ |\n",
    "| 1 | Must be unique                | `id`                                                             | Passed: All 1470 values are unique             |\n",
    "| 2 | Must be between 18 and 60     | `age`                                                            | Passed: No out-of-range values                 |\n",
    "| 3 | Must be in allowed set        | `education` ∈ {Below College, College, Bachelor, Master, Doctor} | Passed: All values valid                       |\n",
    "| 4 | Must be int or float          | `daily_rate`                                                     | Passed: Observed type was `int64`              |\n",
    "| 5 | `monthly_rate` > `daily_rate` | `monthly_rate`, `daily_rate`                                     | Passed: All 1470 rows satisfied this condition |\n",
    "| 6 | Median between 4000–6000      | `monthly_income`                                                 | Passed: Median = 4919.0                        |\n",
    "| 7 | Row count between 1000–2000   | Entire dataset                                                   | Passed: 1470 rows observed                     |\n",
    "\n",
    "Each expectation returned `success: true`, confirming that **no violations** were found and **no manual intervention** was needed. These validations were saved to an **expectation suite** and made persistent using a **named checkpoint** (`checkpoint_m3`), allowing this entire validation logic to be re-executed reliably in future data pipelines. Finally, the results were rendered into human-readable **Data Docs**, providing a visual audit trail for data quality review and documentation.\n",
    "\n",
    "### B. Business Impact\n",
    "\n",
    "Clean, validated data is more than a technical necessity as it enables **trusted insights** that inform **strategic decisions**. With this validated HR dataset, stakeholders can confidently explore:\n",
    "\n",
    "* Key drivers of employee attrition\n",
    "* Role or department-level performance trends\n",
    "* Early indicators of disengagement\n",
    "\n",
    "This ensures that downstream visualizations, dashboards, and models are **built on solid ground**, supporting impactful decisions in HR policy, workforce planning, and organizational strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
